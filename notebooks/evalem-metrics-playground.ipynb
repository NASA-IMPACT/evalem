{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6aa4d3c-ceb4-45b4-8b94-d12575a1c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8accef27-c294-445b-9f8b-2865cb6f5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bec53dc4-af47-4332-b349-2ffd545e46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalem._base.structures import (\n",
    "    PredictionDTO,\n",
    "    ReferenceDTO,\n",
    "    EvaluationDTO,\n",
    "    PredictionInstance,\n",
    "    ReferenceInstance\n",
    ")\n",
    "from evalem.misc.utils import format_to_jury"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aacb3d5-5b52-4108-9ccd-b439e25dbfde",
   "metadata": {},
   "source": [
    "# Generate dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee86c8d-09cb-447b-bcb4-1a9978f8442f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reference 1', 'Reference 2']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single References (SR)\n",
    "references = [\n",
    "    ReferenceDTO(value=\"Reference 1\"),\n",
    "    ReferenceDTO(value=\"Reference 2\")\n",
    "]\n",
    "format_to_jury(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "982d7fc6-8529-4aa0-9634-207289cecb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Reference 1', 'Reference 1.1', 'Reference 2'],\n",
       " ['Reference 2', 'Dummy 1', 'Dummy 2', 'Dummy 3']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiple References (MR)\n",
    "references = [\n",
    "    [\n",
    "        ReferenceDTO(\"Reference 1\"),\n",
    "        dict(value=\"Reference 1.1\"),\n",
    "                dict(value=\"Reference 2\"),\n",
    "\n",
    "    ],\n",
    "    [\n",
    "        dict(value=\"Reference 2\"),\n",
    "        ReferenceDTO(\"Dummy 1\"),\n",
    "        ReferenceDTO(\"Dummy 2\"),\n",
    "        \"Dummy 3\"\n",
    "    ]\n",
    "]\n",
    "format_to_jury(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d32f89e1-66e9-4ace-8bc0-6ef09318fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\n",
    "    PredictionDTO(value=\"Reference 1\", score=1.0),\n",
    "    PredictionDTO(value=\"Reference 2.5\", score=0.75)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3397e7-780e-489b-bbb4-c620cd079835",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "All the evaluation metric takes in same format (references and predictions).\n",
    "And each metric results in same data structure of MetricResult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f100bde4-bc51-4fb5-bab8-a2b413055a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalem._base.evaluators import (\n",
    "    Evaluator,\n",
    ")\n",
    "from evalem._base.structures import MetricResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a15037a-6877-4925-8c0f-50ac1d305abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base metrics\n",
    "from evalem._base.metrics import (\n",
    "    Metric,\n",
    "    JuryBasedMetric,\n",
    "    AccuracyMetric,\n",
    "    PrecisionMetric,\n",
    "    RecallMetric,\n",
    "    F1Metric,\n",
    "    BasicMetric,\n",
    "    ConfusionMatrix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9399e6be-65ce-47f0-bf00-1f85099f952c",
   "metadata": {},
   "source": [
    "## Basic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cad9b414-d68e-4924-9913-db38030a609e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetricResult(score=0.8333333333333333,\n",
      "             total_items=2,\n",
      "             metric_name='PrecisionMetric',\n",
      "             empty_items=0,\n",
      "             extra={'precision': {'score': 0.8333333333333333}})\n"
     ]
    }
   ],
   "source": [
    "pprint(PrecisionMetric()(references=references, predictions=predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f3df9c1-9f04-4b81-864d-6dc87c0de589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetricResult(score=1.0,\n",
      "             total_items=2,\n",
      "             metric_name='RecallMetric',\n",
      "             empty_items=0,\n",
      "             extra={'recall': {'score': 1.0}})\n"
     ]
    }
   ],
   "source": [
    "pprint(RecallMetric()(references=references, predictions=predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a747e8e2-b1b0-41ef-92de-888e5bbee6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetricResult(score=0.8333333333333333,\n",
      "             total_items=2,\n",
      "             metric_name='AccuracyMetric',\n",
      "             empty_items=0,\n",
      "             extra={'accuracy': {'score': 0.8333333333333333}})\n"
     ]
    }
   ],
   "source": [
    "pprint(AccuracyMetric()(references=references, predictions=predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2fd0788-43ce-456c-95c4-49aaa524a5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetricResult(score=None,\n",
      "             total_items=7,\n",
      "             metric_name='ConfusionMatrix',\n",
      "             empty_items=0,\n",
      "             extra={'confusion_matrix': array([[0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0]]),\n",
      "                    'flattened': True,\n",
      "                    'labels': ['Dummy 1',\n",
      "                               'Dummy 2',\n",
      "                               'Dummy 3',\n",
      "                               'Reference 1',\n",
      "                               'Reference 1.1',\n",
      "                               'Reference 2',\n",
      "                               'Reference 2.5']})\n"
     ]
    }
   ],
   "source": [
    "pprint(ConfusionMatrix()(references=references, predictions=predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bc502a-26c4-48f3-ab0d-4355c56945cd",
   "metadata": {},
   "source": [
    "## Wrap with evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "414aad68-c564-42c4-9786-5188387ea040",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(metrics=[\n",
    "    PrecisionMetric(),\n",
    "    RecallMetric(),\n",
    "    F1Metric(),\n",
    "    AccuracyMetric()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8644ea9-701a-4863-b047-bcaac01ca6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MetricResult(score=0.8333333333333333,\n",
      "              total_items=2,\n",
      "              metric_name='PrecisionMetric',\n",
      "              empty_items=0,\n",
      "              extra={'precision': {'score': 0.8333333333333333}}),\n",
      " MetricResult(score=1.0,\n",
      "              total_items=2,\n",
      "              metric_name='RecallMetric',\n",
      "              empty_items=0,\n",
      "              extra={'recall': {'score': 1.0}}),\n",
      " MetricResult(score=0.9,\n",
      "              total_items=2,\n",
      "              metric_name='F1Metric',\n",
      "              empty_items=0,\n",
      "              extra={'f1': {'score': 0.9}}),\n",
      " MetricResult(score=0.8333333333333333,\n",
      "              total_items=2,\n",
      "              metric_name='AccuracyMetric',\n",
      "              empty_items=0,\n",
      "              extra={'accuracy': {'score': 0.8333333333333333}})]\n"
     ]
    }
   ],
   "source": [
    "pprint(evaluator(references=references, predictions=predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f337b-8f74-4d5d-9942-b6f4cbece439",
   "metadata": {},
   "source": [
    "## NLP metric\n",
    "\n",
    "All the NLP metrics are derived from `NLPMetric` base class (which is directly inherited from `evalem._base.Metric`).\n",
    "\n",
    "`evalem.nlp.SemanticMetric` represents semantic metrics which are bert scores and the likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27f98420-043a-4ea4-9828-3f20be319bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalem.nlp.metrics import NLPMetric, SemanticMetric\n",
    "\n",
    "from evalem.nlp.metrics import (\n",
    "    BartScore,\n",
    "    BertScore,\n",
    "    BleuMetric,\n",
    "    ExactMatchMetric,\n",
    "    MeteorMetric,\n",
    "    RougeMetric,\n",
    "    SacreBleuMetric,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0150d7c2-ead6-4a66-a479-4c6d061d64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [\n",
    "    \"I love NLP\",\n",
    "    \"I love working with language models\",\n",
    "    \"I love my cat\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad2ca4a0-93ac-4816-a658-8a0d3774fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\n",
    "    \"I don't really like doing NLP\",\n",
    "    \"Language models are okay\",\n",
    "    \"I absolutely love my cat\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40b03d55-9ade-4214-911a-c26903101b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetricResult(score=0.7106098333994547,\n",
      "             total_items=3,\n",
      "             metric_name='BertScore',\n",
      "             empty_items=0,\n",
      "             extra={'bertscore': {'f1': 0.7106098333994547,\n",
      "                                  'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.28.1)',\n",
      "                                  'precision': 0.6862475474675497,\n",
      "                                  'recall': 0.7450003822644552,\n",
      "                                  'score': 0.7106098333994547}})\n"
     ]
    }
   ],
   "source": [
    "pprint(BertScore(device=\"cpu\")(references=references, predictions=predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4642c5aa-9d81-4940-b757-a53eba9b372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetricResult(score=0.7106098333994547,\n",
      "             total_items=3,\n",
      "             metric_name='BertScore',\n",
      "             empty_items=0,\n",
      "             extra={'bertscore': {'f1': 0.7106098333994547,\n",
      "                                  'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.28.1)',\n",
      "                                  'precision': 0.6862475474675497,\n",
      "                                  'recall': 0.7450003822644552,\n",
      "                                  'score': 0.7106098333994547}})\n"
     ]
    }
   ],
   "source": [
    "pprint(BertScore(device=\"cpu\")(references=references, predictions=predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2503789-e570-4600-b4ed-517d1c01e264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/npantha/dev/impact/projects/evalem/venv/lib/python3.10/site-packages/jury/metrics/_core/auto.py:106: UserWarning: Metric exact_match is not available on jury, falling back to evaluate metric. You may not fully utilize this metric for different input types, e.g multiple predictions or multiple references.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetricResult(score=0.0,\n",
      "             total_items=3,\n",
      "             metric_name='ExactMatchMetric',\n",
      "             empty_items=0,\n",
      "             extra={'exact_match': 0.0, 'flattened': True})\n"
     ]
    }
   ],
   "source": [
    "pprint(ExactMatchMetric()(references=references, predictions=predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f2d8c26-f3b7-4e28-8d7c-235250eb012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetricResult(score=0.5106758851564175,\n",
      "             total_items=3,\n",
      "             metric_name='MeteorMetric',\n",
      "             empty_items=0,\n",
      "             extra={'meteor': {'score': 0.5106758851564175}})\n"
     ]
    }
   ],
   "source": [
    "pprint(MeteorMetric()(references=references, predictions=predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41db5a71-8aa7-4f95-850b-0b962071c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_nlp = Evaluator(metrics=[\n",
    "    BertScore(device=\"cpu\"),\n",
    "    BartScore(device=\"cpu\"),\n",
    "    ExactMatchMetric(),\n",
    "    MeteorMetric(),\n",
    "    RougeMetric(),\n",
    "    SacreBleuMetric(),\n",
    "    BleuMetric()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "013f4d68-4ecd-48f1-bf83-5005d66da5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluator_nlp(references=references, predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1c68bc8-0f79-4313-921f-493870f2ab94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MetricResult(score=0.7106098333994547,\n",
      "              total_items=3,\n",
      "              metric_name='BertScore',\n",
      "              empty_items=0,\n",
      "              extra={'bertscore': {'f1': 0.7106098333994547,\n",
      "                                   'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.28.1)',\n",
      "                                   'precision': 0.6862475474675497,\n",
      "                                   'recall': 0.7450003822644552,\n",
      "                                   'score': 0.7106098333994547}}),\n",
      " MetricResult(score=-3.517256816228231,\n",
      "              total_items=3,\n",
      "              metric_name='BartScore',\n",
      "              empty_items=0,\n",
      "              extra={'flattened': True,\n",
      "                     'model_checkpoint': 'bartscore-large-cnn'}),\n",
      " MetricResult(score=0.0,\n",
      "              total_items=3,\n",
      "              metric_name='ExactMatchMetric',\n",
      "              empty_items=0,\n",
      "              extra={'exact_match': 0.0, 'flattened': True}),\n",
      " MetricResult(score=0.5106758851564175,\n",
      "              total_items=3,\n",
      "              metric_name='MeteorMetric',\n",
      "              empty_items=0,\n",
      "              extra={'meteor': {'score': 0.5106758851564175}}),\n",
      " MetricResult(score=0.4906746031746032,\n",
      "              total_items=3,\n",
      "              metric_name='RougeMetric',\n",
      "              empty_items=0,\n",
      "              extra={'rouge': {'rouge1': 0.562962962962963,\n",
      "                               'rouge2': 0.27380952380952384,\n",
      "                               'rougeL': 0.562962962962963,\n",
      "                               'rougeLsum': 0.562962962962963}}),\n",
      " MetricResult(score=0.09652434877402244,\n",
      "              total_items=3,\n",
      "              metric_name='SacreBleuMetric',\n",
      "              empty_items=0,\n",
      "              extra={'sacrebleu': {'bp': 1.0,\n",
      "                                   'counts': [2, 0, 0, 0],\n",
      "                                   'precisions': [0.33333333333333337,\n",
      "                                                  0.1,\n",
      "                                                  0.0625,\n",
      "                                                  0.04166666666666667],\n",
      "                                   'ref_len': 6,\n",
      "                                   'score': 0.09652434877402244,\n",
      "                                   'sys_len': 6,\n",
      "                                   'totals': [6, 5, 4, 3]}}),\n",
      " MetricResult(score=0.0,\n",
      "              total_items=3,\n",
      "              metric_name='BleuMetric',\n",
      "              empty_items=0,\n",
      "              extra={'bleu': {'brevity_penalty': 1.0,\n",
      "                              'length_ratio': 1.2307692307692308,\n",
      "                              'precisions': [0.5,\n",
      "                                             0.23076923076923078,\n",
      "                                             0.1,\n",
      "                                             0.0],\n",
      "                              'reference_length': 13,\n",
      "                              'score': 0.0,\n",
      "                              'translation_length': 16}})]\n"
     ]
    }
   ],
   "source": [
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b903ab6-b295-4fa7-bc91-8ffad31924c9",
   "metadata": {},
   "source": [
    "# Model wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a946b3d4-607c-4872-848c-68e9a6f669f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead7e9a1-14e3-4b8f-b62e-6278794c0c69",
   "metadata": {},
   "source": [
    "## QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4ddd591-820b-4638-b3fe-f16b5e3be584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalem.nlp.models import QuestionAnsweringHFPipelineWrapper\n",
    "from evalem.nlp.structures import QuestionAnsweringDTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35049726-ab49-4894-b3ff-faffe47d527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1bfeab4-8a06-404d-aca5-55720e8b4940",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe0f22fa-f3e9-4b5b-9b81-ea13ae85eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model = QuestionAnsweringHFPipelineWrapper(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"mps\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98f05c72-a158-4a6f-a79f-d1d5c49dc456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random gpt-4 generated samples for testing\n",
    "data = [\n",
    "    {\n",
    "        \"context\": \"Deep within the labyrinthine caves, echoes tell tales of lost civilizations.\",\n",
    "        \"question\": \"What sails toward the unknown?\",\n",
    "        \"reference\": \"A lone ship\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Beneath the bustling city streets, forgotten catacombs hold secrets of the past.\",\n",
    "        \"question\": \"What watches the skies?\",\n",
    "        \"reference\": \"The old observatory\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"In the heart of the ancient forest, a hidden lake glimmers under the moonlight.\",\n",
    "        \"question\": \"What lies in the heart of the forest?\",\n",
    "        \"reference\": \"A hidden lake\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Beneath the bustling city streets, forgotten catacombs hold secrets of the past.\",\n",
    "        \"question\": \"What lies in the heart of the forest?\",\n",
    "        \"reference\": \"Forgotten catacombs\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Deep within the labyrinthine caves, echoes tell tales of lost civilizations.\",\n",
    "        \"question\": \"What lies in the heart of the forest?\",\n",
    "        \"reference\": \"Labyrinthine caves\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d02265f-bb43-486d-b593-79e06e7353b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[QuestionAnsweringDTO(value='Deep within the labyrinthine caves, echoes tell tales of lost civilizations', score=0.27572375535964966, start=0, end=75, context=None, question=None),\n",
       " QuestionAnsweringDTO(value='forgotten catacombs hold secrets of the past', score=0.2389649599790573, start=35, end=79, context=None, question=None),\n",
       " QuestionAnsweringDTO(value='a hidden lake glimmers under the moonlight', score=0.08825943619012833, start=36, end=78, context=None, question=None),\n",
       " QuestionAnsweringDTO(value='forgotten catacombs hold secrets of the past', score=0.2389649599790573, start=35, end=79, context=None, question=None),\n",
       " QuestionAnsweringDTO(value='Deep within the labyrinthine caves, echoes tell tales of lost civilizations', score=0.27572375535964966, start=0, end=75, context=None, question=None)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "wrapped_model([ dict(context=d[\"context\"], question=d[\"context\"]) for d in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eca567-8050-427c-adc2-78bfd2163363",
   "metadata": {},
   "source": [
    "## QA Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b4cb763-95b2-468c-98d2-229cd6b416b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalem import SimpleEvaluationPipeline\n",
    "from evalem.nlp.evaluators import QAEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9682f132-70e3-4a99-bd7c-892ebf2be8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pipe = SimpleEvaluationPipeline(\n",
    "    model=wrapped_model,\n",
    "    evaluators=[QAEvaluator()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "750e9894-b83c-46c8-a638-c66f7d4d5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [ dict(context=d[\"context\"], question=d[\"context\"]) for d in data]\n",
    "references = list(map(lambda x: x[\"reference\"], data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26d6e539-7cfb-4433-a274-65573b29a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can directly run the evaluator by providing references and predictions\n",
    "result = QAEvaluator()(\n",
    "    references=references,\n",
    "    predictions=wrapped_model(inputs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a49744ab-8a84-452d-af02-cf80dac29b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or the evaluation pipeline will take care of the forward pass\n",
    "results = eval_pipe(\n",
    "    inputs,\n",
    "    references,\n",
    "    model_params=dict(batch_size=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5376e13f-559e-4836-9703-7cf8c00cc269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[MetricResult(score=0.20779220779220778,\n",
      "               total_items=5,\n",
      "               metric_name='AccuracyMetric',\n",
      "               empty_items=0,\n",
      "               extra={'accuracy': {'score': 0.20779220779220778}}),\n",
      "  MetricResult(score=0.0,\n",
      "               total_items=5,\n",
      "               metric_name='ExactMatchMetric',\n",
      "               empty_items=0,\n",
      "               extra={'exact_match': 0.0, 'flattened': True}),\n",
      "  MetricResult(score=0.3168316831683168,\n",
      "               total_items=5,\n",
      "               metric_name='F1Metric',\n",
      "               empty_items=0,\n",
      "               extra={'f1': {'score': 0.3168316831683168}})]]\n"
     ]
    }
   ],
   "source": [
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f396bb-0be7-4efe-8dd1-f942c9223ae7",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac9f19e6-8d1d-47cd-8168-ccf332ec58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee7fe733-6fe2-4581-b5a9-7895e3a661c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalem.nlp.models import TextClassificationHFPipelineWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b4bbdec-d9d6-4181-961f-39d50f4a0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8c5bd44-4ee8-43d1-aebd-1b129ff67d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3de8f35d-9249-46bb-a4cd-614764298fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model = TextClassificationHFPipelineWrapper(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "adc43c8f-4822-4ed3-a330-e227effe3c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ClassificationDTO(value='POSITIVE', score=0.9972068667411804)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_model(\"I do like mangoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e9c7f8-60ec-49a9-a9df-0dfd1e7af6de",
   "metadata": {},
   "source": [
    "## Text Classification Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a58edd4e-efeb-462b-9acc-ecad3299afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalem.nlp.evaluators import TextClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ded0ca3-1704-4c0a-893e-86542bf9054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalem.nlp.misc.datasets import get_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39b7c8dd-4316-4ad7-9b81-7a0a28ef24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"I love NLP\",\n",
    "    \"I love working with language models\",\n",
    "    \"I love my cat\",\n",
    "    \"I don't like mangoes\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    \"POSITIVE\",\n",
    "    \"POSITIVE\",\n",
    "    \"POSITIVE\",\n",
    "    \"NEGATIVE\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3a99de4-beec-40a5-9088-aa0a3141abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = get_imdb(\"test\", nsamples=25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7da8970-ead4-4c21-9bbb-043772a99079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ClassificationDTO(value='POSITIVE', score=0.9997692704200745),\n",
       " ClassificationDTO(value='POSITIVE', score=0.9984956979751587),\n",
       " ClassificationDTO(value='POSITIVE', score=0.9998416900634766),\n",
       " ClassificationDTO(value='NEGATIVE', score=0.9931724667549133)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "221422cf-1486-41c9-97fb-5ee91cfba8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = TextClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef6a942b-07e8-4a24-ad9a-c71dc8b63401",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluator(\n",
    "    references=references,\n",
    "    predictions=wrapped_model(inputs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1f60410-ad1d-4d1b-9798-ca1b41aa0f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MetricResult(score=1.0,\n",
      "              total_items=4,\n",
      "              metric_name='AccuracyMetric',\n",
      "              empty_items=0,\n",
      "              extra={'accuracy': {'score': 1.0}}),\n",
      " MetricResult(score=1.0,\n",
      "              total_items=4,\n",
      "              metric_name='F1Metric',\n",
      "              empty_items=0,\n",
      "              extra={'f1': {'score': 1.0}}),\n",
      " MetricResult(score=1.0,\n",
      "              total_items=4,\n",
      "              metric_name='PrecisionMetric',\n",
      "              empty_items=0,\n",
      "              extra={'precision': {'score': 1.0}}),\n",
      " MetricResult(score=1.0,\n",
      "              total_items=4,\n",
      "              metric_name='RecallMetric',\n",
      "              empty_items=0,\n",
      "              extra={'recall': {'score': 1.0}}),\n",
      " MetricResult(score=None,\n",
      "              total_items=4,\n",
      "              metric_name='ConfusionMatrix',\n",
      "              empty_items=0,\n",
      "              extra={'confusion_matrix': array([[1, 0],\n",
      "       [0, 3]]),\n",
      "                     'flattened': True,\n",
      "                     'labels': ['NEGATIVE', 'POSITIVE']})]\n"
     ]
    }
   ],
   "source": [
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d339399e-59f8-4e2a-8302-4cb4a99c70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of manually doing forward pass for wrapped model, wrap it up\n",
    "eval_pipe = SimpleEvaluationPipeline(\n",
    "    model=wrapped_model,\n",
    "    evaluators=[TextClassificationEvaluator()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51032f19-1a16-4b66-a90a-3c4aae5b90bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = eval_pipe(inputs, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "181558ab-f266-4d13-bab9-bf972b082a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[MetricResult(score=1.0,\n",
      "               total_items=4,\n",
      "               metric_name='AccuracyMetric',\n",
      "               empty_items=0,\n",
      "               extra={'accuracy': {'score': 1.0}}),\n",
      "  MetricResult(score=1.0,\n",
      "               total_items=4,\n",
      "               metric_name='F1Metric',\n",
      "               empty_items=0,\n",
      "               extra={'f1': {'score': 1.0}}),\n",
      "  MetricResult(score=1.0,\n",
      "               total_items=4,\n",
      "               metric_name='PrecisionMetric',\n",
      "               empty_items=0,\n",
      "               extra={'precision': {'score': 1.0}}),\n",
      "  MetricResult(score=1.0,\n",
      "               total_items=4,\n",
      "               metric_name='RecallMetric',\n",
      "               empty_items=0,\n",
      "               extra={'recall': {'score': 1.0}}),\n",
      "  MetricResult(score=None,\n",
      "               total_items=4,\n",
      "               metric_name='ConfusionMatrix',\n",
      "               empty_items=0,\n",
      "               extra={'confusion_matrix': array([[1, 0],\n",
      "       [0, 3]]),\n",
      "                      'flattened': True,\n",
      "                      'labels': ['NEGATIVE', 'POSITIVE']})]]\n"
     ]
    }
   ],
   "source": [
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cbde55-8090-4819-b851-7e08722b6656",
   "metadata": {},
   "source": [
    "# Compose eny evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe1fc782-c85b-42ae-9f89-775f476f1a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/npantha/dev/impact/projects/evalem/venv/lib/python3.10/site-packages/jury/metrics/_core/auto.py:106: UserWarning: Metric exact_match is not available on jury, falling back to evaluate metric. You may not fully utilize this metric for different input types, e.g multiple predictions or multiple references.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "evaluators = [\n",
    "    Evaluator(metrics=[\n",
    "        AccuracyMetric(),\n",
    "        ConfusionMatrix(),\n",
    "        ExactMatchMetric(),\n",
    "        F1Metric(),\n",
    "    ]),\n",
    "    Evaluator(metrics=[\n",
    "        BertScore(),\n",
    "        BartScore()\n",
    "    ])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b7a0c774-1dc1-4413-943b-997c469b8664",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pipe = SimpleEvaluationPipeline(\n",
    "    model = wrapped_model,\n",
    "    evaluators=evaluators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e488276-a8d5-4bcf-aa73-66dbe5db5712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[MetricResult(score=1.0,\n",
      "               total_items=4,\n",
      "               metric_name='AccuracyMetric',\n",
      "               empty_items=0,\n",
      "               extra={'accuracy': {'score': 1.0}}),\n",
      "  MetricResult(score=None,\n",
      "               total_items=4,\n",
      "               metric_name='ConfusionMatrix',\n",
      "               empty_items=0,\n",
      "               extra={'confusion_matrix': array([[1, 0],\n",
      "       [0, 3]]),\n",
      "                      'flattened': True,\n",
      "                      'labels': ['NEGATIVE', 'POSITIVE']}),\n",
      "  MetricResult(score=1.0,\n",
      "               total_items=4,\n",
      "               metric_name='ExactMatchMetric',\n",
      "               empty_items=0,\n",
      "               extra={'exact_match': 1.0, 'flattened': True}),\n",
      "  MetricResult(score=1.0,\n",
      "               total_items=4,\n",
      "               metric_name='F1Metric',\n",
      "               empty_items=0,\n",
      "               extra={'f1': {'score': 1.0}})],\n",
      " [MetricResult(score=0.9999999552965164,\n",
      "               total_items=4,\n",
      "               metric_name='BertScore',\n",
      "               empty_items=0,\n",
      "               extra={'bertscore': {'f1': 0.9999999552965164,\n",
      "                                    'hashcode': 'bert-base-uncased_L9_no-idf_version=0.3.12(hug_trans=4.28.1)',\n",
      "                                    'precision': 0.9999999552965164,\n",
      "                                    'recall': 0.9999999552965164,\n",
      "                                    'score': 0.9999999552965164}}),\n",
      "  MetricResult(score=-1.36616450548172,\n",
      "               total_items=4,\n",
      "               metric_name='BartScore',\n",
      "               empty_items=0,\n",
      "               extra={'flattened': True,\n",
      "                      'model_checkpoint': 'bartscore-large-cnn'})]]\n"
     ]
    }
   ],
   "source": [
    "pprint(eval_pipe(inputs, references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c290c3-20a2-4e90-b203-6a9415b9a5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
